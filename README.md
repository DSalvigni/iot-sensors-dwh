# sensor-iot-dwh

# Software configuration setup
1. DB -> mariadb-10.3.8-winx64.msi
2. Connectors -> MariaDB Connector/ODBC 3.0.6 Windows Installers (mariadb-connector-odbc-3.0.6-win64.msi)
3. Modelling -> MySQL WOrkbench 8.0 CE 64 bit
4. WINVER -> Windows 10, Version 1803 (OS Build 17134.164) - 64 Bit 
5. Machine -> Laptop AMD A4-6210 APU
6. Python 3.x (Plus LIBs in the PY script)
7. HeidiSQL 9.5.0.5 (Export DB/TBLs creation)

# How it works
In my assumption we have a buch of swithes, and each of them has 2 components liked, and each component has 5 sensors installed. 
The configuration of the sensors (ex Frequence of data collection) can change.
I keep the idea simple so in the data series (TIMESTAMP-VALUE), I added a 5 chars code (PT001,PT002) which rapresents the single sensor. Each sensor is unique and by this id can be linked to the swithes information.
They generate file in a folder (./../DATA) monitored by an events-- handler class-oriented script, written in python: any time a new file comes in, it is cathed, put in queue, and uploaded.
The configuration table keeps track of the sensor setup by using a time range (Valid from-to). 
The time range extremes are used to filter the data in the time series, in order to get generated values in the period of the requested configuration.
The model created rapresents the transactional idealist world (ER 3FN not to replicate data) and the operational environment which is in my mind a kind of DWH, where to extract or import the data, and this is what I implemented and coded.
I populated the DB with some dummy data to test the queries.

# Folder description
- DATA: It cointains 5 samples of data used by the python script to be automatically uploaded. 
        Once the script is running it is sufficent to copy and paste these files to generate upload events. 
		The files cointain the name of the sensor per line (ACCZ,ACCY...etc), and in its name, and it used by python to understand in which table these values must be saved;
		Each file as mentioned cointains the SEONSOR-ID(sen_platform_code), per each line of time series, which identify the sensor per switch component: it is a unique value in my assumption.
- MODEL: It cointains a 3FN ER model and an Operational model (WinBench required). The models have been exported in pdf also. The models are 2:
  -> PDF_3FN_ER_Model for the perfect description of the Businees Case (It has been not added a calendar table but can be used the internal DB one) 
  -> PDF_Denormalized_ER_Model for the operational environment. This one has been implemented in the DB, with data replication but for better analytical reasons (Less join, Denormalized master data etc...)
- SQLDB: In this folder you can find a DB SQL dump, with demo data of the DB and the query requested in the business case.
  -> sdi.sql: Database dump (just copy and paste in your MariaDB client) with demo data
  -> Query01/02/03.sql: The required query. Be carefull to replace the interval in the where condition for the date. -2 days work today, but tomorrow must be changed in -3 etc...Unless to insert fresh data
- TEST: It cointains a version of the Python file where I removed the replicate "path" enclosing them in a global variable (Still in test).
- data.log: Data log used by Python to track all the creation files activity
- watchdog_sensordata.py: Python script which must be setup (configure the path where the /DATA folder is). Once launched it monitor that new files are coming in the DATA folder.  
  Every time the file arrives it raises an event to launch the fast "load data local infile" command.

# Note
1. All the Path as much as possible are absolute and windows standard. Be carefull to run Python and LOAD script in other environment because the intepretation of / and \ can be inverted
2. Date are forced to be in format 'YYYY-MM-DD hh:mm:ss' and in the SQL I add a TRIM for "empty return" generated by winds ('\r\n')
3. The switch GEO Loc (GIS) has been saved in POINT datatype so to be queried must be used the funciton ST_AsText() (Ex. Select ST_AsText(swi_gis))
3. It could be necessary in same case to have properly working the MySQL LIBs, the Visual C++ Redistributable package (Depending on the OS version used)
4. No indexes have been enabled on the sensor tables in order to keep performance high. Aria DB (But InnoDB works well also) have been setup. 
   I set up the following variable to 0 to keep performance high:
	- set unique_checks = 0;
	- set foreign_key_checks = 0;
	- set sql_log_bin=0;
5. Possible Enhancehments: 
   - instead of watchdog it could be possible to go more in detail by using the EXECUTOR class in C# to create a data pipeline by following the patter of Producer-Consumer (Like KAFKA/Zookeper).
   - If the amount of TXT data increase it could be to think to use the NoSQL solution (MongoDB) to collect the vertical time series and then import them in an ETL process in MariaDB
   - Generally I do not import the data directly in operational table but I park them in an ODS staging area: in this case you can see that the sensor data are collected in table with ID-VALUE-TS and they go directly in tables with final data format (TimeStamp).
	 To increase the performance it could be possible to remove the sen_platform_code and keep it in the name of the file and to import the data in N-Tables defined as varchar-columns only, which are reflecting the number of sensors used (100/200 or more...): on this is the consept has been founded the shredding of data approach.
	 After this must be implemented and ETL internal to the DB to move the data and cast them (date/numbers) in final sensors tables, by creating the unique index (which is part of the creating file name)
